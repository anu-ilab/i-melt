class Model(object):
    """
    derived from  https://github.com/adventuresinML/adventures-in-ml-code/blob/master/weight_init_tensorflow.py
    """
    def __init__(self, input_size, initialization, activation, num_layers=3,
                 hidden_size=100):
        self._input_size = input_size
        self._init = initialization
        self._activation = activation
        # num layers does not include the input layer
        self._num_layers = num_layers
        self._hidden_size = hidden_size
        self._init_bias = np.array([-4.5,np.log(50000.),np.log(5.1),10.,0.0001])
        self._model_def()
    
    def _visco_out(self,T):
            return self.ae + self.be / (T * (self.sctg + (self.ap * (tf.log(T)-tf.log(self.tg)) + self.b * (T-self.tg))))
        
    def _calculate_variance(self, x):
        mean = tf.reduce_mean(x)
        sqr = tf.square(x - mean)
        return tf.reduce_mean(sqr)
    
    def _model_def(self):
        # create placeholder variables
        self.input_c = tf.placeholder(dtype=tf.float32, shape=[None,self._input_size], name="chimie")
        self.input_T = tf.placeholder(dtype=tf.float32, shape=[None,1], name="T")
        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None,1], name="viscosity")

        # create self._num_layers dense layers as the model
        input = self.input_c
        tf.summary.scalar("input_var", self._calculate_variance(input))
        for i in range(self._num_layers - 1):
            print('layer{}'.format(i+1))
            input = tf.layers.dense(input, self._hidden_size, kernel_initializer=self._init,
                                    activation=self._activation, name='layer{}'.format(i+1))
            # get the input to the nodes (sans bias)
            mat_mul_in = tf.get_default_graph().get_tensor_by_name("layer{}/MatMul:0".format(i + 1))
            # log pre and post activation function histograms
            tf.summary.histogram("mat_mul_hist_{}".format(i + 1), mat_mul_in)
            tf.summary.histogram("fc_out_{}".format(i + 1), input)
            # also log the variance of mat mul
            tf.summary.scalar("mat_mul_var_{}".format(i + 1), self._calculate_variance(mat_mul_in))
        
        self.output = tf.layers.dense(input, 5,bias_initializer=tf.constant_initializer(self._init_bias), 
                                      name='layer{}'.format(self._num_layers))
        
        mat_mul_in = tf.get_default_graph().get_tensor_by_name("layer{}/MatMul:0".format(self._num_layers))
        tf.summary.histogram("mat_mul_hist_{}".format(self._num_layers), mat_mul_in)
        tf.summary.histogram("fc_out_{}".format(self._num_layers), input)
        
        #
        # Adam and Gibbs with network outputs
        #
        self.ae = tf.placeholder(dtype=tf.float32, shape=[None,1], name="Ae")
        self.be = tf.placeholder(dtype=tf.float32, shape=[None,1], name="Be")
        self.sctg = tf.placeholder(dtype=tf.float32, shape=[None,1], name="ScTg")
        self.ap = tf.placeholder(dtype=tf.float32, shape=[None,1], name="ap")
        self.b = tf.placeholder(dtype=tf.float32, shape=[None,1], name="b")

        self.ae, self.be, self.sctg, self.ap, self.b = tf.split(self.output,5,axis=1)

        # cannot be negative
        self.be = tf.exp(self.be)
        self.sctg = tf.exp(self.sctg)
        self.ap = tf.exp(self.ap)
        self.b = tf.exp(self.b)
        
        self.tg = self.be/((12.0-self.ae)*self.sctg)
        
        self.visco_pred = self.ae + self.be / (self.input_T * (self.sctg + (self.ap * (tf.log(self.input_T)-tf.log(self.tg)) + self.b * (self.input_T-self.tg))))
        
        self.loss = tf.nn.l2_loss(self.visco_pred-self.input_y)
                                                                            
        # add the loss to the summary
        tf.summary.scalar('loss', self.loss)
        self.optimizer = tf.train.RMSPropOptimizer(1e-4).minimize(self.loss)
        #self.accuracy = self._compute_accuracy(self.visco_pred, self.input_y)
        #tf.summary.scalar('acc', self.accuracy)
        self.merged = tf.summary.merge_all()
        self.init_op = tf.global_variables_initializer()
        
    def _init_pass_through(model, base_path, fold, chimie, T, visco):
        with tf.Session() as sess:
            sess.run(model.init_op)
            train_writer = tf.summary.FileWriter(base_path + fold,
                                                 sess.graph)
            summary = sess.run(model.merged, feed_dict={model.input_c: chimie,
                                                        model.input_T: T.reshape(-1,1),
                                                        model.input_y: visco.reshape(-1,1)})
            train_writer.add_summary(summary, 0)

    def train(self, base_path, fold, epochs, chimie, T, visco):
    
        with tf.Session() as sess:
            sess.run(self.init_op)
            train_writer = tf.summary.FileWriter(base_path + fold,
                                             sess.graph)
            for i in range(epochs):
                loss, _ = sess.run([self.loss, self.optimizer],
                                        feed_dict={self.input_c: chimie,
                                                            self.input_T: T.reshape(-1,1),
                                                            self.input_y: visco.reshape(-1,1)})
                if i % 50 == 0:
                    print("Iteration {} of {} - loss: {:.3f}%".
                          format(i, epochs, loss))
                    summary = sess.run(self.merged, feed_dict={self.input_c: chimie,
                                                            self.input_T: T.reshape(-1,1),
                                                            self.input_y: visco.reshape(-1,1)})
                train_writer.add_summary(summary, i)
            
    def predict(self, chimie, T, visco):
    
        with tf.Session() as sess:
            sess.run(self.init_op)
            return sess.run([self.visco_pred,self.ae, self.be, self.sctg, self.ap, self.b, self.tg], 
                            feed_dict={self.input_c: chimie,
                                        self.input_T: T.reshape(-1,1),
                                        self.input_y: visco.reshape(-1,1)})
           
           
           
           
#
# First sequential code
#

try:
    sess.close()
except:
    print("no session yet")

    
NIN = chimie_train.shape[1]
NHIDDEN = 40
STDEV = 0.01
NOUT = 5 # Ae, Be, ScTg, a, b

c = tf.placeholder(dtype=tf.float32, shape=[None,NIN], name="chimie")
T = tf.placeholder(dtype=tf.float32, shape=[None,1], name="T")
y = tf.placeholder(dtype=tf.float32, shape=[None,1], name="viscosity")

Wh1 = tf.Variable(tf.random_normal([NIN,NHIDDEN], stddev=1e-3, dtype=tf.float32))
bh1 = tf.Variable(tf.random_normal([1,NHIDDEN], stddev=1e-3, dtype=tf.float32))

Wh2 = tf.Variable(tf.random_normal([NHIDDEN,NHIDDEN], stddev=1e-3, dtype=tf.float32))
bh2 = tf.Variable(tf.random_normal([1,NHIDDEN], stddev=1e-3, dtype=tf.float32))

#Wh3 = tf.Variable(tf.random_normal([NHIDDEN,NHIDDEN], stddev=1e-9, dtype=tf.float32))
#bh3 = tf.Variable(tf.random_normal([1,NHIDDEN], stddev=1e-9, dtype=tf.float32))

Wo = tf.Variable(tf.random_normal([NHIDDEN,NOUT], mean=0.,stddev=1e-3, dtype=tf.float32))
bo = tf.Variable(tf.random_normal([1,NOUT], mean=init_bias, stddev=init_bias*0.001, dtype=tf.float32))

hidden_layer_1 = tf.nn.dropout(tf.nn.relu(tf.matmul(c, Wh1) + bh1),keep_prob=0.9)
hidden_layer_2 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden_layer_1, Wh2) + bh2),keep_prob=0.9)
#hidden_layer_3 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden_layer_2, Wh3) + bh3),keep_prob=0.9)

output = tf.matmul(hidden_layer_2,Wo) + bo

def get_ag_params(output):
    ae = tf.placeholder(dtype=tf.float32, shape=[None,1], name="Ae")
    be = tf.placeholder(dtype=tf.float32, shape=[None,1], name="Be")
    sctg = tf.placeholder(dtype=tf.float32, shape=[None,1], name="ScTg")
    ap = tf.placeholder(dtype=tf.float32, shape=[None,1], name="ap")
    b = tf.placeholder(dtype=tf.float32, shape=[None,1], name="b")
    
    ae, be, sctg, ap, b = tf.split(output,5,axis=1)

    # cannot be negative
    be = tf.exp(be)
    ap = tf.exp(ap)
    b = tf.exp(b)
    sctg = tf.exp(sctg)

    return ae, be, sctg, ap, b

ae, be, sctg, ap, b = get_ag_params(output)

tg = be/((12.0-ae)*sctg)

def visco_out(ae, be, sctg, ap, b, tg, T):
    return ae + be / (T * (sctg + (ap * (tf.log(T)-tf.log(tg)) + b * (T-tg))))

visco_pred = visco_out(ae, be, sctg, ap, b, tg, T)

lossfunc = tf.nn.l2_loss(visco_pred-y)
#lossfunc = tf.losses.mean_squared_error(visco_pred,y)

train_op = tf.train.RMSPropOptimizer(1e-4).minimize(lossfunc)

sess = tf.InteractiveSession()
sess.run(tf.initialize_all_variables())

pred_ini= sess.run(visco_pred,feed_dict={c: chimie_train, T: temperature_train, y: y_train.reshape(-1,1)})
plt.figure()
plt.plot(y_train, pred_ini)