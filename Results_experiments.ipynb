{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA AVAILABLE?  True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# (c) Charles Le Losq 2021\n",
    "# see embedded licence file\n",
    "\n",
    "# Library loading\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd # manipulate dataframes\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch, time\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# First we check if CUDA is available\n",
    "print(\"CUDA AVAILABLE? \",torch.cuda.is_available())\n",
    "      \n",
    "device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "import imelt\n",
    "\n",
    "# scaling coefficients for loss function\n",
    "# viscosity is always one\n",
    "# scaling coefficients for loss function\n",
    "# viscosity is always one\n",
    "ls = imelt.loss_scales()        \n",
    "entro_scale = ls.entro\n",
    "raman_scale = ls.raman\n",
    "density_scale = ls.density\n",
    "ri_scale = ls.ri\n",
    "tg_scale = ls.tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of dataset analysis\n",
    "\n",
    "Here we trained 10 networks for each train dataset size.\n",
    "\n",
    "We will select the best ones for each dataset size, and report the RMSE for viscosity for the training and validation reference data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9)\n"
     ]
    }
   ],
   "source": [
    "# Data import\n",
    "prefix= [\"0p10val\",\"0p20val\",\"0p30val\",\"0p40val\",\"0p50val\",\"0p60val\",\"0p70val\",\"0p80val\"]\n",
    "\n",
    "suffix = [\"_0\",\"_1\",\"_2\",\"_3\",\"_4\",\"_5\",\"_6\",\"_7\",\"_8\",\"_9\"]\n",
    "\n",
    "# variable to record the results\n",
    "total_size_train = np.zeros(len(prefix))\n",
    "compo_size_train = np.zeros(len(prefix))\n",
    "# order for errors is AG, MYEGA, A-M, CG, TVF, Raman, Density, Entropy, Refractivce Index\n",
    "error_size_train = np.zeros((len(prefix),len(suffix),9))\n",
    "error_size_valid = np.zeros((len(prefix),len(suffix),9))\n",
    "\n",
    "# scaling coefficients\n",
    "scaling = np.array([[1., 1., 1., 1., 1., raman_scale, density_scale, entro_scale, ri_scale]])\n",
    "print(scaling.shape)\n",
    "\n",
    "# Loading the reference dataset\n",
    "ds = imelt.data_loader(\"./data/NKAS_viscosity_reference.hdf5\",\n",
    "                             \"./data/NKAS_Raman.hdf5\",\n",
    "                             \"./data/NKAS_density.hdf5\",\n",
    "                             \"./data/NKAS_optical.hdf5\",\n",
    "                             device)\n",
    "\n",
    "# Loss criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for j in range(len(prefix)): # main loop: between datasets\n",
    "\n",
    "    # Load the dataset used for training, to get the number of compositions\n",
    "    dataset_name = \"./data/NKAS_viscosity_{}.hdf5\".format(prefix[j])\n",
    "    ds_ = imelt.data_loader(dataset_name,\n",
    "                             \"./data/NKAS_Raman.hdf5\",\n",
    "                             \"./data/NKAS_density.hdf5\",\n",
    "                             \"./data/NKAS_optical.hdf5\",\n",
    "                             device)\n",
    "    \n",
    "    # saving in output variables\n",
    "    total_size_train[j] = len(ds_.T_visco_train)\n",
    "    compo_size_train[j] = np.unique(ds_.x_visco_train,axis=0).shape[0]\n",
    "    \n",
    "    for i in range(len(suffix)): # subloop: between each different training on one dataset\n",
    "        \n",
    "        # Loading the network and freezing dropout\n",
    "        name_model = \"model_l4_n200_p0_data{}{}.pth\".format(prefix[j],suffix[i])\n",
    "        neuralmodel = imelt.model(4,200,4,ds.nb_channels_raman,p_drop=0.01) \n",
    "        neuralmodel.load_state_dict(torch.load(\"./model/exp_trainsize/\"+name_model, map_location='cpu'))\n",
    "        neuralmodel.eval()\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        with torch.set_grad_enabled(False):\n",
    "    \n",
    "            # train\n",
    "            y_ag_pred_train = neuralmodel.ag(ds.x_visco_train,ds.T_visco_train)\n",
    "            y_myega_pred_train = neuralmodel.myega(ds.x_visco_train,ds.T_visco_train)\n",
    "            y_am_pred_train = neuralmodel.am(ds.x_visco_train,ds.T_visco_train)\n",
    "            y_cg_pred_train = neuralmodel.cg(ds.x_visco_train,ds.T_visco_train)\n",
    "            y_tvf_pred_train = neuralmodel.tvf(ds.x_visco_train,ds.T_visco_train)\n",
    "            y_raman_pred_train = neuralmodel.raman_pred(ds.x_raman_train)\n",
    "            y_density_pred_train = neuralmodel.density(ds.x_density_train)\n",
    "            y_entro_pred_train = neuralmodel.sctg(ds.x_entro_train)\n",
    "            y_ri_pred_train = neuralmodel.sellmeier(ds.x_ri_train, ds.lbd_ri_train)\n",
    "\n",
    "            # valid\n",
    "            y_ag_pred_valid = neuralmodel.ag(ds.x_visco_valid,ds.T_visco_valid)\n",
    "            y_myega_pred_valid = neuralmodel.myega(ds.x_visco_valid,ds.T_visco_valid)\n",
    "            y_am_pred_valid = neuralmodel.am(ds.x_visco_valid,ds.T_visco_valid)\n",
    "            y_cg_pred_valid = neuralmodel.cg(ds.x_visco_valid,ds.T_visco_valid)\n",
    "            y_tvf_pred_valid = neuralmodel.tvf(ds.x_visco_valid,ds.T_visco_valid)\n",
    "            y_raman_pred_valid = neuralmodel.raman_pred(ds.x_raman_valid)\n",
    "            y_density_pred_valid = neuralmodel.density(ds.x_density_valid)\n",
    "            y_entro_pred_valid = neuralmodel.sctg(ds.x_entro_valid)\n",
    "            y_ri_pred_valid = neuralmodel.sellmeier(ds.x_ri_valid, ds.lbd_ri_valid)\n",
    "\n",
    "            # Compute Loss\n",
    "\n",
    "            # train \n",
    "            error_size_train[j,i,0]  = np.sqrt(criterion(y_ag_pred_train, ds.y_visco_train).item())\n",
    "            error_size_train[j,i,1]  = np.sqrt(criterion(y_myega_pred_train, ds.y_visco_train).item())\n",
    "            error_size_train[j,i,2]  = np.sqrt(criterion(y_am_pred_train, ds.y_visco_train).item())\n",
    "            error_size_train[j,i,3]  = np.sqrt(criterion(y_raman_pred_train,ds.y_raman_train).item())\n",
    "            error_size_train[j,i,4]  = np.sqrt(criterion(y_density_pred_train,ds.y_density_train).item())\n",
    "            error_size_train[j,i,5]  = np.sqrt(criterion(y_entro_pred_train,ds.y_entro_train).item())\n",
    "\n",
    "            # validation\n",
    "            error_size_valid[j,i,0] = np.sqrt(criterion(y_ag_pred_valid, ds.y_visco_valid).item())\n",
    "            error_size_valid[j,i,1] = np.sqrt(criterion(y_myega_pred_valid, ds.y_visco_valid).item())\n",
    "            error_size_valid[j,i,2] = np.sqrt(criterion(y_am_pred_valid, ds.y_visco_valid).item())\n",
    "            error_size_valid[j,i,3] = np.sqrt(criterion(y_raman_pred_valid,ds.y_raman_valid).item())\n",
    "            error_size_valid[j,i,4] = np.sqrt(criterion(y_density_pred_valid,ds.y_density_valid).item())\n",
    "            error_size_valid[j,i,5] = np.sqrt(criterion(y_entro_pred_valid,ds.y_entro_valid).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for model:\n\tsize mismatch for out_raman.weight: copying a param with shape torch.Size([870, 100]) from checkpoint, the shape in current model is torch.Size([850, 100]).\n\tsize mismatch for out_raman.bias: copying a param with shape torch.Size([870]) from checkpoint, the shape in current model is torch.Size([850]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c4bde1fb2c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Declare model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mneuralmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimelt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_neurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_channels_raman\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mneuralmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model/exp_arch/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mneuralmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for model:\n\tsize mismatch for out_raman.weight: copying a param with shape torch.Size([870, 100]) from checkpoint, the shape in current model is torch.Size([850, 100]).\n\tsize mismatch for out_raman.bias: copying a param with shape torch.Size([870]) from checkpoint, the shape in current model is torch.Size([850])."
     ]
    }
   ],
   "source": [
    "# List of networks\n",
    "list_models = os.listdir(\"./model/exp_arch/\")\n",
    "nb_exp = len(list_models)\n",
    "\n",
    "record_loss = pd.DataFrame()\n",
    "\n",
    "record_loss[\"name\"] = list_models\n",
    "\n",
    "record_loss[\"nb_layers\"] = np.zeros(nb_exp)\n",
    "record_loss[\"nb_neurons\"] = np.zeros(nb_exp)\n",
    "record_loss[\"p_drop\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_ag_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_ag_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_am_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_am_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_am_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_am_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_cg_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_cg_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_tvf_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_tvf_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_Sconf_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_Sconf_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_d_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_d_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_raman_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_raman_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "### Load dataset\n",
    "\n",
    "path_data = \"./data/NKAS_viscosity_reference.hdf5\"\n",
    "path_raman = \"./data/NKAS_Raman.hdf5\"\n",
    "path_density = \"./data/NKAS_density.hdf5\"\n",
    "path_optical = \"./data/NKAS_optical.hdf5\"\n",
    "\n",
    "ds = imelt.data_loader(path_data,path_raman,path_density,path_optical,device)\n",
    "\n",
    "# Loss criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Load dataset\n",
    "for idx,name in enumerate(list_models):\n",
    "    \n",
    "    # Extract arch\n",
    "    nb_layers = int(name[name.find(\"l\")+1:name.find(\"_\")])\n",
    "    nb_neurons = int(name[name.find(\"n\")+1:name.rfind(\"_\")])\n",
    "    p_drop = float(name[name.find(\"p\")+1:name.rfind(\".\")])\n",
    "    \n",
    "    # Record arch\n",
    "    record_loss.loc[idx,\"nb_layers\"] = nb_layers\n",
    "    record_loss.loc[idx,\"nb_neurons\"] = nb_neurons\n",
    "    record_loss.loc[idx,\"p_drop\"] = p_drop\n",
    "    \n",
    "    # Declare model\n",
    "    neuralmodel = imelt.model(4,nb_neurons,nb_layers,ds.nb_channels_raman,p_drop=p_drop) \n",
    "    neuralmodel.load_state_dict(torch.load(\"./model/exp_arch/\"+name, map_location='cpu'))\n",
    "    neuralmodel.eval()\n",
    "    \n",
    "    # PREDICTIONS\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # train\n",
    "        y_ag_pred_train = neuralmodel.ag(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_myega_pred_train = neuralmodel.myega(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_am_pred_train = neuralmodel.am(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_cg_pred_train = neuralmodel.cg(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_tvf_pred_train = neuralmodel.tvf(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_raman_pred_train = neuralmodel.raman_pred(ds.x_raman_train)\n",
    "        y_density_pred_train = neuralmodel.density(ds.x_density_train)\n",
    "        y_entro_pred_train = neuralmodel.sctg(ds.x_entro_train)\n",
    "        y_ri_pred_train = neuralmodel.sellmeier(ds.x_ri_train, ds.lbd_ri_train)\n",
    "\n",
    "        # valid\n",
    "        y_ag_pred_valid = neuralmodel.ag(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_myega_pred_valid = neuralmodel.myega(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_am_pred_valid = neuralmodel.am(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_cg_pred_valid = neuralmodel.cg(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_tvf_pred_valid = neuralmodel.tvf(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_density_pred_valid = neuralmodel.density(ds.x_density_valid)\n",
    "        y_entro_pred_valid = neuralmodel.sctg(ds.x_entro_valid)\n",
    "        y_ri_pred_valid = neuralmodel.sellmeier(ds.x_ri_valid, ds.lbd_ri_valid)\n",
    "\n",
    "        # Compute Loss\n",
    "\n",
    "        # train \n",
    "        record_loss.loc[idx,\"loss_ag_train\"] = np.sqrt(criterion(y_ag_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[idx,\"loss_myega_train\"]  = np.sqrt(criterion(y_myega_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[idx,\"loss_am_train\"]  = np.sqrt(criterion(y_am_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[idx,\"loss_cg_train\"]  = np.sqrt(criterion(y_cg_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[idx,\"loss_tvf_train\"]  = np.sqrt(criterion(y_tvf_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[idx,\"loss_raman_train\"]  = np.sqrt(criterion(y_raman_pred_train,ds.y_raman_train).item())\n",
    "        record_loss.loc[idx,\"loss_d_train\"]  = np.sqrt(criterion(y_density_pred_train,ds.y_density_train).item())\n",
    "        record_loss.loc[idx,\"loss_Sconf_train\"]  = np.sqrt(criterion(y_entro_pred_train,ds.y_entro_train).item())\n",
    "        record_loss.loc[idx,\"loss_ri_train\"]  = np.sqrt(criterion(y_ri_pred_train,ds.y_ri_train).item())\n",
    "\n",
    "        # validation\n",
    "        record_loss.loc[idx,\"loss_ag_valid\"] = np.sqrt(criterion(y_ag_pred_valid, ds.y_visco_valid).item())\n",
    "        record_loss.loc[idx,\"loss_myega_valid\"] = np.sqrt(criterion(y_myega_pred_valid, ds.y_visco_valid).item())\n",
    "        record_loss.loc[idx,\"loss_am_valid\"] = np.sqrt(criterion(y_am_pred_valid, ds.y_visco_valid).item())\n",
    "        record_loss.loc[idx,\"loss_cg_valid\"]  = np.sqrt(criterion(y_cg_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[idx,\"loss_tvf_valid\"]  = np.sqrt(criterion(y_tvf_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[idx,\"loss_raman_valid\"] = np.sqrt(criterion(y_raman_pred_valid,ds.y_raman_valid).item())\n",
    "        record_loss.loc[idx,\"loss_d_valid\"] = np.sqrt(criterion(y_density_pred_valid,ds.y_density_valid).item())\n",
    "        record_loss.loc[idx,\"loss_Sconf_valid\"] = np.sqrt(criterion(y_entro_pred_valid,ds.y_entro_valid).item())\n",
    "        record_loss.loc[idx,\"loss_ri_valid\"]  = np.sqrt(criterion(y_ri_pred_valid,ds.y_ri_valid).item())\n",
    "    \n",
    "    record_loss.loc[idx,\"loss_train\"] = (record_loss.loc[idx,\"loss_ag_train\"] + \n",
    "                                         record_loss.loc[idx,\"loss_myega_train\"] + \n",
    "                                         record_loss.loc[idx,\"loss_am_train\"] + \n",
    "                                         record_loss.loc[idx,\"loss_cg_train\"] + \n",
    "                                         record_loss.loc[idx,\"loss_tvf_train\"] + \n",
    "                                         raman_scale*record_loss.loc[idx,\"loss_raman_train\"] + \n",
    "                                         density_scale*record_loss.loc[idx,\"loss_d_train\"] + \n",
    "                                         entro_scale*record_loss.loc[idx,\"loss_Sconf_train\"] + \n",
    "                                         ri_scale*record_loss.loc[idx,\"loss_ri_train\"])\n",
    "    \n",
    "    record_loss.loc[idx,\"loss_valid\"] = (record_loss.loc[idx,\"loss_ag_valid\"] + \n",
    "                                         record_loss.loc[idx,\"loss_myega_valid\"] + \n",
    "                                         record_loss.loc[idx,\"loss_am_valid\"] + \n",
    "                                         record_loss.loc[idx,\"loss_cg_valid\"] + \n",
    "                                         record_loss.loc[idx,\"loss_tvf_valid\"] + \n",
    "                                         raman_scale*record_loss.loc[idx,\"loss_raman_valid\"] + \n",
    "                                         density_scale*record_loss.loc[idx,\"loss_d_valid\"] + \n",
    "                                         entro_scale*record_loss.loc[idx,\"loss_Sconf_valid\"] + \n",
    "                                         ri_scale*record_loss.loc[idx,\"loss_ri_valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figure 1 : architecture and training dataset size tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "ax = plt.subplot(3,2,1)\n",
    "\n",
    "plt.errorbar(compo_size_train, \n",
    "             np.median(error_size_train[:,:,1], axis=1),\n",
    "             yerr=np.std(error_size_train[:,:,1], axis=1),fmt=\"s\", color=\"C0\", label=\"Training subset\")\n",
    "plt.errorbar(compo_size_train, \n",
    "             np.median(error_size_valid[:,:,1], axis=1),\n",
    "             yerr=np.std(error_size_valid[:,:,1], axis=1),fmt=\"o\", color=\"C1\", label = \"Validation subset\")\n",
    "plt.plot([0,150],[0.6,0.6],\"k--\")\n",
    "#plt.xlim(10,120)\n",
    "\n",
    "plt.annotate(\"(a)\",xy=(0.95,0.9),xycoords=\"axes fraction\",ha=\"right\")\n",
    "plt.xlabel(\"Num. of compositions in training subset of $D_{viscosity}$\")\n",
    "plt.ylabel(\"RMSE $\\eta$ (eq. 1), log Pa$\\cdot$s\")\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plt.plot(record_loss.loc[:,\"nb_neurons\"]*record_loss.loc[:,\"nb_layers\"],record_loss.loc[:,\"loss_train\"],\n",
    "         \"s\",alpha=0.05,ms=2,color=\"C0\")\n",
    "plt.plot(record_loss.loc[:,\"nb_neurons\"]*record_loss.loc[:,\"nb_layers\"],record_loss.loc[:,\"loss_valid\"],\n",
    "         \"o\",alpha=0.05,ms=2,color=\"C1\")\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Total number of hidden neurons\")\n",
    "plt.ylabel(\"Global loss value, a. u.\")\n",
    "plt.annotate(\"(b)\",xy=(0.95,0.85),xycoords=\"axes fraction\",ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.violinplot([record_loss.loc[record_loss[\"nb_layers\"]==i,\"loss_train\"].values for i in range(1,7)],\n",
    "               [i for i in range(1,7)])\n",
    "\n",
    "plt.violinplot([record_loss.loc[record_loss[\"nb_layers\"]==i,\"loss_valid\"].values for i in range(1,7)],\n",
    "               [i for i in range(1,7)])\n",
    "\n",
    "plt.yscale('log')\n",
    "#plt.ylim(0.01,10000)\n",
    "\n",
    "plt.xlabel(\"Number of layers\")\n",
    "plt.ylabel(\"Global loss value, a. u.\")\n",
    "plt.annotate(\"(c)\",xy=(0.95,0.9),xycoords=\"axes fraction\",ha=\"right\")\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.plot(record_loss.loc[:,\"nb_neurons\"],record_loss.loc[:,\"loss_train\"],\"s\",alpha=0.05,markersize=2,color=\"C0\")\n",
    "plt.plot(record_loss.loc[:,\"nb_neurons\"],record_loss.loc[:,\"loss_valid\"],\"o\",alpha=0.05,markersize=2,color=\"C1\")\n",
    "plt.yscale('log')\n",
    "\n",
    "#plt.ylim(0.01,10000)\n",
    "\n",
    "plt.xlabel(\"Number of neurons per layer\")\n",
    "plt.ylabel(\"Global loss value, a. u.\")\n",
    "           \n",
    "plt.annotate(\"(d)\",xy=(0.95,0.9),xycoords=\"axes fraction\",ha=\"right\")\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "plt.plot(record_loss.loc[:,\"p_drop\"],record_loss.loc[:,\"loss_train\"],\"s\",color=\"C0\",alpha=0.05,markersize=2,label=\"Training subset\")\n",
    "plt.plot(record_loss.loc[:,\"p_drop\"],record_loss.loc[:,\"loss_valid\"],\"o\",color=\"C1\",alpha=0.05,markersize=2,label=\"Validation subset\")\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "plt.ylim(10,10000)\n",
    "\n",
    "plt.annotate(\"(e)\",xy=(0.95,0.9),xycoords=\"axes fraction\",ha=\"right\")\n",
    "\n",
    "plt.xlabel(\"Dropout probability\")\n",
    "plt.ylabel(\"Global loss value, a. u.\")\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.legend(fancybox=True,bbox_to_anchor=(2.1, -2.))\n",
    "\n",
    "plt.savefig(\"./figures/SupplementaryFigure_1_architecture.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and save best models\n",
    "\n",
    "For that we use the global \"loss_valid\" = loss_viscosity + loss_raman + loss_density + loss_refractiveindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nb_layers</th>\n",
       "      <th>nb_neurons</th>\n",
       "      <th>p_drop</th>\n",
       "      <th>loss_ag_train</th>\n",
       "      <th>loss_ag_valid</th>\n",
       "      <th>loss_am_train</th>\n",
       "      <th>loss_am_valid</th>\n",
       "      <th>loss_cg_train</th>\n",
       "      <th>loss_cg_valid</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_d_train</th>\n",
       "      <th>loss_d_valid</th>\n",
       "      <th>loss_raman_train</th>\n",
       "      <th>loss_raman_valid</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_valid</th>\n",
       "      <th>loss_myega_train</th>\n",
       "      <th>loss_ri_train</th>\n",
       "      <th>loss_myega_valid</th>\n",
       "      <th>loss_ri_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>l5_n399_p0.03.pth</td>\n",
       "      <td>5.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.255427</td>\n",
       "      <td>0.682354</td>\n",
       "      <td>0.246003</td>\n",
       "      <td>0.725449</td>\n",
       "      <td>0.252160</td>\n",
       "      <td>0.252160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>0.074422</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>43.331710</td>\n",
       "      <td>56.150502</td>\n",
       "      <td>0.270387</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.798081</td>\n",
       "      <td>0.003368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>l5_n319_p0.05.pth</td>\n",
       "      <td>5.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.283869</td>\n",
       "      <td>0.629838</td>\n",
       "      <td>0.260360</td>\n",
       "      <td>0.667443</td>\n",
       "      <td>0.278039</td>\n",
       "      <td>0.278039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010985</td>\n",
       "      <td>0.016870</td>\n",
       "      <td>0.080372</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>45.635131</td>\n",
       "      <td>59.569026</td>\n",
       "      <td>0.299317</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.775361</td>\n",
       "      <td>0.003684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>l5_n229_p0.03.pth</td>\n",
       "      <td>5.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.271847</td>\n",
       "      <td>0.658131</td>\n",
       "      <td>0.255071</td>\n",
       "      <td>0.690859</td>\n",
       "      <td>0.274824</td>\n",
       "      <td>0.274824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.081955</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>43.320981</td>\n",
       "      <td>61.449183</td>\n",
       "      <td>0.300795</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.780368</td>\n",
       "      <td>0.003889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>l4_n481_p0.05.pth</td>\n",
       "      <td>4.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.271260</td>\n",
       "      <td>0.707777</td>\n",
       "      <td>0.259017</td>\n",
       "      <td>0.756576</td>\n",
       "      <td>0.253647</td>\n",
       "      <td>0.253647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010234</td>\n",
       "      <td>0.015885</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>43.488091</td>\n",
       "      <td>61.895669</td>\n",
       "      <td>0.286444</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.840964</td>\n",
       "      <td>0.004012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>l4_n200_p0.09.pth</td>\n",
       "      <td>4.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.284874</td>\n",
       "      <td>0.726760</td>\n",
       "      <td>0.268609</td>\n",
       "      <td>0.767657</td>\n",
       "      <td>0.279573</td>\n",
       "      <td>0.279573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>44.574612</td>\n",
       "      <td>61.935433</td>\n",
       "      <td>0.309084</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.874315</td>\n",
       "      <td>0.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>l4_n487_p0.07.pth</td>\n",
       "      <td>4.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.280456</td>\n",
       "      <td>0.603491</td>\n",
       "      <td>0.275758</td>\n",
       "      <td>0.635678</td>\n",
       "      <td>0.275390</td>\n",
       "      <td>0.275390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>0.015140</td>\n",
       "      <td>0.074886</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>45.317876</td>\n",
       "      <td>63.604948</td>\n",
       "      <td>0.291175</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.734836</td>\n",
       "      <td>0.004282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>l5_n350_p0.03.pth</td>\n",
       "      <td>5.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.276438</td>\n",
       "      <td>0.733888</td>\n",
       "      <td>0.255624</td>\n",
       "      <td>0.779965</td>\n",
       "      <td>0.256858</td>\n",
       "      <td>0.256858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>0.075067</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>42.235180</td>\n",
       "      <td>64.470453</td>\n",
       "      <td>0.286040</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.860033</td>\n",
       "      <td>0.004089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>l5_n96_p0.02.pth</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.291217</td>\n",
       "      <td>0.651596</td>\n",
       "      <td>0.273507</td>\n",
       "      <td>0.681631</td>\n",
       "      <td>0.271250</td>\n",
       "      <td>0.271250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.083214</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>44.331742</td>\n",
       "      <td>64.787626</td>\n",
       "      <td>0.299904</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.777534</td>\n",
       "      <td>0.003960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>l5_n451_p0.02.pth</td>\n",
       "      <td>5.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.239256</td>\n",
       "      <td>0.641936</td>\n",
       "      <td>0.237748</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>0.222897</td>\n",
       "      <td>0.222897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.062664</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>38.626937</td>\n",
       "      <td>65.247093</td>\n",
       "      <td>0.244952</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.746029</td>\n",
       "      <td>0.004198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>l5_n410_p0.05.pth</td>\n",
       "      <td>5.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.262558</td>\n",
       "      <td>0.655028</td>\n",
       "      <td>0.264439</td>\n",
       "      <td>0.698953</td>\n",
       "      <td>0.271545</td>\n",
       "      <td>0.271545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>0.016263</td>\n",
       "      <td>0.073911</td>\n",
       "      <td>0.101083</td>\n",
       "      <td>42.955375</td>\n",
       "      <td>65.300006</td>\n",
       "      <td>0.287761</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.781205</td>\n",
       "      <td>0.004313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  nb_layers  nb_neurons  p_drop  loss_ag_train  \\\n",
       "2007  l5_n399_p0.03.pth        5.0       399.0    0.03       0.255427   \n",
       "1057  l5_n319_p0.05.pth        5.0       319.0    0.05       0.283869   \n",
       "1647  l5_n229_p0.03.pth        5.0       229.0    0.03       0.271847   \n",
       "2661  l4_n481_p0.05.pth        4.0       481.0    0.05       0.271260   \n",
       "408   l4_n200_p0.09.pth        4.0       200.0    0.09       0.284874   \n",
       "1530  l4_n487_p0.07.pth        4.0       487.0    0.07       0.280456   \n",
       "1262  l5_n350_p0.03.pth        5.0       350.0    0.03       0.276438   \n",
       "89     l5_n96_p0.02.pth        5.0        96.0    0.02       0.291217   \n",
       "2660  l5_n451_p0.02.pth        5.0       451.0    0.02       0.239256   \n",
       "2837  l5_n410_p0.05.pth        5.0       410.0    0.05       0.262558   \n",
       "\n",
       "      loss_ag_valid  loss_am_train  loss_am_valid  loss_cg_train  \\\n",
       "2007       0.682354       0.246003       0.725449       0.252160   \n",
       "1057       0.629838       0.260360       0.667443       0.278039   \n",
       "1647       0.658131       0.255071       0.690859       0.274824   \n",
       "2661       0.707777       0.259017       0.756576       0.253647   \n",
       "408        0.726760       0.268609       0.767657       0.279573   \n",
       "1530       0.603491       0.275758       0.635678       0.275390   \n",
       "1262       0.733888       0.255624       0.779965       0.256858   \n",
       "89         0.651596       0.273507       0.681631       0.271250   \n",
       "2660       0.641936       0.237748       0.683292       0.222897   \n",
       "2837       0.655028       0.264439       0.698953       0.271545   \n",
       "\n",
       "      loss_cg_valid  ...  loss_d_train  loss_d_valid  loss_raman_train  \\\n",
       "2007       0.252160  ...      0.010713      0.016579          0.074422   \n",
       "1057       0.278039  ...      0.010985      0.016870          0.080372   \n",
       "1647       0.274824  ...      0.009918      0.016685          0.081955   \n",
       "2661       0.253647  ...      0.010234      0.015885          0.065839   \n",
       "408        0.279573  ...      0.010522      0.018901          0.072184   \n",
       "1530       0.275390  ...      0.011678      0.015140          0.074886   \n",
       "1262       0.256858  ...      0.009784      0.017765          0.075067   \n",
       "89         0.271250  ...      0.010184      0.019246          0.083214   \n",
       "2660       0.222897  ...      0.008524      0.017696          0.062664   \n",
       "2837       0.271545  ...      0.010913      0.016263          0.073911   \n",
       "\n",
       "      loss_raman_valid  loss_train  loss_valid  loss_myega_train  \\\n",
       "2007          0.101083   43.331710   56.150502          0.270387   \n",
       "1057          0.101083   45.635131   59.569026          0.299317   \n",
       "1647          0.101083   43.320981   61.449183          0.300795   \n",
       "2661          0.101083   43.488091   61.895669          0.286444   \n",
       "408           0.101083   44.574612   61.935433          0.309084   \n",
       "1530          0.101083   45.317876   63.604948          0.291175   \n",
       "1262          0.101083   42.235180   64.470453          0.286040   \n",
       "89            0.101083   44.331742   64.787626          0.299904   \n",
       "2660          0.101083   38.626937   65.247093          0.244952   \n",
       "2837          0.101083   42.955375   65.300006          0.287761   \n",
       "\n",
       "      loss_ri_train  loss_myega_valid  loss_ri_valid  \n",
       "2007       0.002950          0.798081       0.003368  \n",
       "1057       0.003125          0.775361       0.003684  \n",
       "1647       0.003000          0.780368       0.003889  \n",
       "2661       0.003025          0.840964       0.004012  \n",
       "408        0.003070          0.874315       0.003693  \n",
       "1530       0.003030          0.734836       0.004282  \n",
       "1262       0.002919          0.860033       0.004089  \n",
       "89         0.003057          0.777534       0.003960  \n",
       "2660       0.002739          0.746029       0.004198  \n",
       "2837       0.002882          0.781205       0.004313  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_recorded = record_loss[(record_loss.nb_layers>2)&(record_loss.nb_layers<6)].nsmallest(10,\"loss_valid\")\n",
    "best_recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing shutil module  \n",
    "import shutil \n",
    "\n",
    "# Copy the content of \n",
    "# source to destination \n",
    "for i in best_recorded.loc[:,\"name\"]:\n",
    "    shutil.copyfile(\"./model/exp_arch/\"+i, \"./model/best/\"+i) \n",
    "\n",
    "best_recorded.to_csv(\"./model/best/best_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
